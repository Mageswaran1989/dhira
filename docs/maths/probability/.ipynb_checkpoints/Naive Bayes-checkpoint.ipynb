{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes?\n",
    "This interpretation of probability that we use belongs to the category called Bayesian\n",
    "probability; it’s popular and it works well. Bayesian probability is named after Thomas\n",
    "Bayes, who was an eighteenth-century theologian. Bayesian probability allows prior\n",
    "knowledge and logic to be applied to uncertain statements. There’s another\n",
    "interpretation called frequency probability, which only draws conclusions from data\n",
    "and doesn’t allow for logic and prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory:\n",
    "An abstract illustration of the procedure used by the naive Bayes classifier to choose the topic for a document. In the training corpus, most documents are automotive, so the classifier starts out at a point closer to the “automotive” label. But it then considers the effect of each feature. In this example, the input document contains the word “dark,” which is a weak indicator for murder mysteries, but it also contains the word “football,” which is a strong indicator for sports documents. After every feature has made its contribution, the classifier checks which label it is closest to, and assigns that label to the input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![here](../images/naive_bayes/naive-bayes-triangle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating label likelihoods with naive Bayes: \n",
    "Naive Bayes begins by calculating the prior probability of each label, based on how frequently each label occurs in the training data. Every feature then contributes to the likelihood estimate for each label, by multiplying it by the probability that input values with that label will have that feature. The resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features, assuming that the feature probabilities are all independent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/naive_bayes/naive_bayes_bargraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics:\n",
    "### Conditional Probability:\n",
    "Let’s assume for a moment that we have a jar containing seven stones.\n",
    "\n",
    "3 Gray and 4 Black Balls .\n",
    "\n",
    "If we stick a hand into this jar and randomly pull out a stone, what are the chances that the stone will be gray?\n",
    "\n",
    "There are 7 possible stones and 3 are gray, so the probability is P(gray) = 3/7.\n",
    "\n",
    "What is the probability of grabbing a black stone? It’s P(black) = 4/7\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/naive_bayes/cp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the seven stones were in two buckets?\n",
    "\n",
    "If you want to calculate the P(gray) or P(black) , would knowing the bucket change the answer? Yes, which is known as conditional probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/naive_bayes/cp1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write this as P(gray|bucketB) , and this would be read as “the prob-\n",
    "ability of gray given bucket B.”\n",
    "\n",
    "It’s not hard to see that P(gray|bucketA) is 2/4 and P(gray|bucketB) is 1/3.\n",
    "\n",
    "To formalize how to calculate the conditional probability, we can say\n",
    "P(gray|bucketB) = P(gray and bucketB)/P(bucketB)\n",
    "\n",
    "i.e P(gray|bucketB)  or P(x|condition) = P(gray and bucketB)/P(bucketB)  = (1/7) / (3/7) = 1/3\n",
    "\n",
    "Another useful way to manipulate conditional probabilities is known as Bayes’ rule.\n",
    "Bayes’ rule tells us how to swap the symbols in a conditional probability statement. If\n",
    "we have P(x|c) but want to have P(c|x) , we can find it with the following:\n",
    "\n",
    "P(c|x) = P(x|c) P(c) / P(x)\n",
    "\n",
    "What is the probability of bucket being A or B if the chosen ball is gray?\n",
    "\n",
    "P(bucketB|gray) = P(gray|bucketB) P(bucketB) / P(gray)\n",
    "\n",
    "                                  = (1/3) (3/7) / (3/4) = 4/21 = 0.1904\n",
    "\n",
    "P(bucketA|gray) = P(gray|bucketA) P(bucketA) / P(gray)\n",
    "\n",
    "                                  = (1/2) (4/7) / (3/4) = 4/14 = 0.2857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Code:\n",
    "### Document classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "Count the number of documents in each class\n",
    "for every training document:\n",
    "    for each class:\n",
    "        if a token appears in the document ➞ increment the count for that token\n",
    "        increment the count for tokens\n",
    "    for each class:\n",
    "        for each token:\n",
    "            divide the token count by the total token count to get conditional probabilities\n",
    "    return conditional probabilities for each class\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    " \n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "              ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "              ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "              ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "              ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "              ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1] #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    " \n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([]) #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    " \n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: \n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec\n",
    " \n",
    "def trainNB0(trainMatrix,trainCategory):  \n",
    "    numTrainDocs = len(trainMatrix)  \n",
    "    numWords = len(trainMatrix[0])  \n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords) #change to ones() \n",
    "    p0Denom = 2.0; p1Denom = 2.0 #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "        p1Vect = log(p1Num/p1Denom) #change to log()\n",
    "        p0Vect = log(p0Num/p0Denom) #change to log()\n",
    "        return p0Vect,p1Vect,pAbusive\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1) #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type:  Supervised Learning   \n",
    "Use cases: Text classification   \n",
    "Pros: Works with a small amount of data, handles multiple classes  \n",
    "Cons: Sensitive to how the input data is prepared   \n",
    "Works with: Nominal values i.e Numeric or Boolean values  \n",
    "Analyse:  Use Histogram to analyse the training   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
